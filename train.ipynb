{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f234c171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'experiment', 'override': True, 'data_folder': 'data/dung_256x256_sle_gan_ready', 'resolution': 256, 'generator_weights': False, 'discriminator_weights': False, 'batch_size': 1, 'epochs': 1000, 'G_learning_rate': '2e-4', 'D_learning_rate': '2e-4', 'diff_augment': True, 'diff_augment_policies': 'translation,cutout', 'fid': False, 'fid_frequency': 1, 'fid_number_of_images': 128}\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "import sle_gan\n",
    "\n",
    "import yaml\n",
    "with open(\"config.yaml\", 'r') as stream:\n",
    "    cfg = yaml.safe_load(stream)\n",
    "\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f3a70fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "_ = [tf.config.experimental.set_memory_growth(x, True) for x in physical_devices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0c215ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders for this experiment\n",
    "# or override the old ones...\n",
    "\n",
    "experiments_folder = Path(\"logs\") / cfg[\"name\"]\n",
    "\n",
    "if experiments_folder.is_dir():\n",
    "    if cfg[\"override\"]:\n",
    "        shutil.rmtree(experiments_folder)\n",
    "    else:\n",
    "        raise FileExistsError(\"Experiment already exists\")\n",
    "        \n",
    "checkpoints_folder = experiments_folder / \"checkpoints\"\n",
    "checkpoints_folder.mkdir(parents=True)\n",
    "\n",
    "logs_folder = experiments_folder / \"tensorboard_logs\"\n",
    "logs_folder.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5ec2d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the image dataset\n",
    "\n",
    "dataset = sle_gan.create_dataset(batch_size=cfg[\"batch_size\"], \n",
    "                                 folder=cfg[\"data_folder\"], \n",
    "                                 resolution=cfg[\"resolution\"],\n",
    "                                 use_flip_augmentation=True, \n",
    "                                 shuffle_buffer_size=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81f0ed33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-13 14:28:32.393054: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model G] output shape: (1, 256, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "# create Generator\n",
    "\n",
    "G = sle_gan.Generator(cfg[\"resolution\"])\n",
    "sample_G_output = G.initialize(cfg[\"batch_size\"])\n",
    "\n",
    "# load G weights if they exist\n",
    "if cfg[\"generator_weights\"] is not False:\n",
    "    G.load_weights(cfg[\"generator_weights\"])\n",
    "    print(\"Weights are loaded for G\")\n",
    "print(f\"[Model G] output shape: {sample_G_output.shape}\")\n",
    "\n",
    "G_optimizer = tf.optimizers.Adam(learning_rate=cfg[\"G_learning_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ba4deb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model D] real_fake output shape: (1, 5, 5, 1)\n",
      "[Model D] image output shape(1, 128, 128, 3)\n",
      "[Model D] image part output shape(1, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "# create Discriminator\n",
    "\n",
    "D = sle_gan.Discriminator(cfg[\"resolution\"])\n",
    "sample_D_output = D.initialize(cfg[\"batch_size\"])\n",
    "\n",
    "if cfg[\"generator_weights\"] is not False:\n",
    "    D.load_weights(str(checkpoints_folder / \"D_checkpoint.h5\"))\n",
    "    print(\"Weights are loaded for D\")\n",
    "print(f\"[Model D] real_fake output shape: {sample_D_output[0].shape}\")\n",
    "print(f\"[Model D] image output shape{sample_D_output[1].shape}\")\n",
    "print(f\"[Model D] image part output shape{sample_D_output[2].shape}\")\n",
    "\n",
    "D_optimizer = tf.optimizers.Adam(learning_rate=cfg[\"D_learning_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ce58ed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_from_path(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ee4a3cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset2(batch_size: int,\n",
    "                   folder: str,\n",
    "                   resolution: int,\n",
    "                   use_flip_augmentation: bool = True,\n",
    "                   image_extension: str = \"jpg\",\n",
    "                   shuffle_buffer_size: int = 100):\n",
    "    dataset = tf.data.Dataset.list_files(folder + f\"/*.{image_extension}\")\n",
    "    dataset = dataset.map(read_image_from_path)\n",
    "    dataset = dataset.map(partial(preprocess_images, resolution=resolution))\n",
    "    dataset = dataset.shuffle(buffer_size=shuffle_buffer_size).batch(batch_size)\n",
    "    \n",
    "    #.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9f4b804f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_images2(batch_size: int, folder: str, resolution: int):\n",
    "    dataset = create_dataset2(batch_size, str(folder), resolution=resolution, use_flip_augmentation=False,\n",
    "                             shuffle_buffer_size=1)\n",
    "    print(dataset.shape)\n",
    "    \n",
    "    #for x in dataset.take(1):\n",
    "    #    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "71aaaf81",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'partial' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zj/f_wv_4b16pd3hxw8qg93mh1m0000gn/T/ipykernel_25486/1686543578.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_test_images2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data_folder\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"resolution\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/zj/f_wv_4b16pd3hxw8qg93mh1m0000gn/T/ipykernel_25486/3046134080.py\u001b[0m in \u001b[0;36mget_test_images2\u001b[0;34m(batch_size, folder, resolution)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_test_images2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresolution\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     dataset = create_dataset2(batch_size, str(folder), resolution=resolution, use_flip_augmentation=False,\n\u001b[0;32m----> 3\u001b[0;31m                              shuffle_buffer_size=1)\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/zj/f_wv_4b16pd3hxw8qg93mh1m0000gn/T/ipykernel_25486/3804202204.py\u001b[0m in \u001b[0;36mcreate_dataset2\u001b[0;34m(batch_size, folder, resolution, use_flip_augmentation, image_extension, shuffle_buffer_size)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf\"/*.{image_extension}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_image_from_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresolution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresolution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle_buffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'partial' is not defined"
     ]
    }
   ],
   "source": [
    "test_images = get_test_images2(2, cfg[\"data_folder\"], cfg[\"resolution\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39a13e28",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Cannot batch tensors with different shapes in component 0. First element had shape [256,256,1] and element 9 had shape [256,256,3]. [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zj/f_wv_4b16pd3hxw8qg93mh1m0000gn/T/ipykernel_25486/2688774774.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtest_input_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtest_input_for_generation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msle_gan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_input_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_input_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtest_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msle_gan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_test_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_input_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data_folder\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"resolution\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# tb_file_writer = tf.summary.create_file_writer(str(logs_folder))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/_DIGITAL/ML/2021/july/sle_dunggan/dung-sle-gan/sle_gan/data.py\u001b[0m in \u001b[0;36mget_test_images\u001b[0;34m(batch_size, folder, resolution)\u001b[0m\n\u001b[1;32m     85\u001b[0m     dataset = create_dataset(batch_size, str(folder), resolution=resolution, use_flip_augmentation=False,\n\u001b[1;32m     86\u001b[0m                              shuffle_buffer_size=1)\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/_DIGITAL/ML/2021/july/sle_dunggan/env/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/_DIGITAL/ML/2021/july/sle_dunggan/env/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    745\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m           output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/_DIGITAL/ML/2021/july/sle_dunggan/env/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2726\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2727\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2728\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2729\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2730\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/_DIGITAL/ML/2021/july/sle_dunggan/env/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6895\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6896\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6897\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6898\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/_DIGITAL/ML/2021/july/sle_dunggan/env/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot batch tensors with different shapes in component 0. First element had shape [256,256,1] and element 9 had shape [256,256,3]. [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "if cfg[\"fid\"]:\n",
    "    # Model for the FID calculation\n",
    "    fid_inception_model = sle_gan.InceptionModel(height=cfg[\"resolution\"], width=cfg[\"resolution\"])\n",
    "\n",
    "# what does this do?\n",
    "test_input_size = 25\n",
    "test_input_for_generation = sle_gan.create_input_noise(test_input_size)\n",
    "test_images = sle_gan.get_test_images(test_input_size, cfg[\"data_folder\"], cfg[\"resolution\"])\n",
    "\n",
    "# tb_file_writer = tf.summary.create_file_writer(str(logs_folder))\n",
    "# tb_file_writer.set_as_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6569c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set loss metrics\n",
    "\n",
    "G_loss_metric = tf.keras.metrics.Mean()\n",
    "D_loss_metric = tf.keras.metrics.Mean()\n",
    "D_real_fake_loss_metric = tf.keras.metrics.Mean()\n",
    "D_I_reconstruction_loss_metric = tf.keras.metrics.Mean()\n",
    "D_I_part_reconstruction_loss_metric = tf.keras.metrics.Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e7c060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set augmentation policies\n",
    "\n",
    "diff_augment_policies = None\n",
    "if cfg[\"diff_augment\"]:\n",
    "    diff_augment_policies = cfg[\"diff_augment_policies\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa1d7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train gan\n",
    "\n",
    "for epoch in range(cfg[\"epochs\"]):\n",
    "    print(\"### Epoch{epoch} ###\".format(epoch))\n",
    "    for step, image_batch in enumerate(dataset):\n",
    "        G_loss, \n",
    "        D_loss, \n",
    "        D_real_fake_loss, \n",
    "        D_I_reconstruction_loss, \n",
    "        D_I_part_reconstruction_loss = sle_gan.train_step(\n",
    "            G=G,\n",
    "            D=D,\n",
    "            G_optimizer=G_optimizer,\n",
    "            D_optimizer=D_optimizer,\n",
    "            images=image_batch,\n",
    "            diff_augmenter_policies=diff_augment_policies)\n",
    "\n",
    "        G_loss_metric(G_loss)\n",
    "        D_loss_metric(D_loss)\n",
    "        D_real_fake_loss_metric(D_real_fake_loss)\n",
    "        D_I_reconstruction_loss_metric(D_I_reconstruction_loss)\n",
    "        D_I_part_reconstruction_loss_metric(D_I_part_reconstruction_loss)\n",
    "\n",
    "        if step % 100 == 0 and step != 0:\n",
    "            print(f\"\\tStep {step} - \"\n",
    "                  f\"G loss {G_loss_metric.result():.4f} | \"\n",
    "                  f\"D loss {D_loss_metric.result():.4f} | \"\n",
    "                  f\"D realfake loss {D_real_fake_loss_metric.result():.4f} | \"\n",
    "                  f\"D I recon loss {D_I_reconstruction_loss_metric.result():.4f} | \"\n",
    "                  f\"D I part recon loss {D_I_part_reconstruction_loss_metric.result():.4f}\")\n",
    "\n",
    "    if cfg[\"fid\"]:\n",
    "        if epoch % cfg[\"fid_frequency\"] == 0:\n",
    "            fid_score = sle_gan.evaluation_step(inception_model=fid_inception_model,\n",
    "                                                dataset=dataset,\n",
    "                                                G=G,\n",
    "                                                batch_size=cfg[\"batch_size\"],\n",
    "                                                image_height=cfg[\"resolution\"],\n",
    "                                                image_width=cfg[\"resolution\"],\n",
    "                                                nb_of_images_to_use=cfg[\"fid_number_of_images\"])\n",
    "            print(f\"[FID] {fid_score:.2f}\")\n",
    "            tf.summary.scalar(\"FID_score\", fid_score, epoch)\n",
    "\n",
    "    tf.summary.scalar(\"G_loss/G_loss\", G_loss_metric.result(), epoch)\n",
    "    tf.summary.scalar(\"D_loss/D_loss\", D_loss_metric.result(), epoch)\n",
    "    tf.summary.scalar(\"D_loss/D_real_fake_loss\", D_real_fake_loss_metric.result(), epoch)\n",
    "    tf.summary.scalar(\"D_loss/D_I_reconstruction_loss\", D_I_reconstruction_loss_metric.result(), epoch)\n",
    "    tf.summary.scalar(\"D_loss/D_I_part_reconstruction_loss\", D_I_part_reconstruction_loss_metric.result(), epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch} - \"\n",
    "          f\"G loss {G_loss_metric.result():.4f} | \"\n",
    "          f\"D loss {D_loss_metric.result():.4f} | \"\n",
    "          f\"D realfake loss {D_real_fake_loss_metric.result():.4f} | \"\n",
    "          f\"D I recon loss {D_I_reconstruction_loss_metric.result():.4f} | \"\n",
    "          f\"D I part recon loss {D_I_part_reconstruction_loss_metric.result():.4f}\")\n",
    "\n",
    "    G_loss_metric.reset_states()\n",
    "    D_loss_metric.reset_states()\n",
    "    D_real_fake_loss_metric.reset_states()\n",
    "    D_I_part_reconstruction_loss_metric.reset_states()\n",
    "    D_I_reconstruction_loss_metric.reset_states()\n",
    "\n",
    "    # TODO: save weights only when the FID score gets better\n",
    "    G.save_weights(str(checkpoints_folder / \"G_checkpoint.h5\"))\n",
    "    D.save_weights(str(checkpoints_folder / \"D_checkpoint.h5\"))\n",
    "\n",
    "    # Generate test images\n",
    "    generated_images = G(test_input_for_generation, training=False)\n",
    "    generated_images = sle_gan.postprocess_images(generated_images, dtype=tf.uint8).numpy()\n",
    "    sle_gan.visualize_images_on_grid_and_save(epoch, generated_images, experiments_folder / \"generated_images\",\n",
    "                                              5, 5)\n",
    "\n",
    "    # Generate reconstructions from Discriminator\n",
    "    _, decoded_images, decoded_part_images = D(test_images, training=False)\n",
    "    decoded_images = sle_gan.postprocess_images(decoded_images, dtype=tf.uint8).numpy()\n",
    "    decoded_part_images = sle_gan.postprocess_images(decoded_part_images, dtype=tf.uint8).numpy()\n",
    "    sle_gan.visualize_images_on_grid_and_save(epoch, decoded_images, experiments_folder / \"reconstructed_whole_images\",\n",
    "                                              5, 5)\n",
    "    sle_gan.visualize_images_on_grid_and_save(epoch, decoded_part_images,\n",
    "                                              experiments_folder / \"reconstructed_part_images\", 5, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

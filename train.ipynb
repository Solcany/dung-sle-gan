{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f234c171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'experiment', 'override': True, 'data_folder': '/', 'resolution': 512, 'generator_weights': False, 'discriminator_weights': False, 'batch_size': 1, 'epochs': 1000, 'G_learning_rate': '2e-4', 'D_learning_rate': '2e-4', 'diff_augment': True, 'fid': True, 'fid_frequency': 1, 'fid_number_of_images': 128}\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "import sle_gan\n",
    "\n",
    "import yaml\n",
    "with open(\"config.yaml\", 'r') as stream:\n",
    "    cfg = yaml.safe_load(stream)\n",
    "\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f3a70fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "_ = [tf.config.experimental.set_memory_growth(x, True) for x in physical_devices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0c215ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders for this experiment\n",
    "# or override the old ones...\n",
    "\n",
    "experiments_folder = Path(\"logs\") / cfg[\"name\"]\n",
    "\n",
    "if experiments_folder.is_dir():\n",
    "    if cfg[\"override\"]:\n",
    "        shutil.rmtree(experiments_folder)\n",
    "    else:\n",
    "        raise FileExistsError(\"Experiment already exists\")\n",
    "        \n",
    "checkpoints_folder = experiments_folder / \"checkpoints\"\n",
    "checkpoints_folder.mkdir(parents=True)\n",
    "\n",
    "logs_folder = experiments_folder / \"tensorboard_logs\"\n",
    "logs_folder.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5ec2d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('logs/experiment')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the image dataset\n",
    "\n",
    "dataset = sle_gan.create_dataset(batch_size=cfg[\"batch_size\"], \n",
    "                                 folder=cfg[\"data_folder\"], \n",
    "                                 resolution=cfg[\"resolution\"],\n",
    "                                 use_flip_augmentation=True, \n",
    "                                 shuffle_buffer_size=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81f0ed33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model G] output shape: (1, 512, 512, 3)\n"
     ]
    }
   ],
   "source": [
    "# create Generator\n",
    "\n",
    "G = sle_gan.Generator(cfg[\"resolution\"])\n",
    "sample_G_output = G.initialize(cfg[\"batch_size\"])\n",
    "\n",
    "# load G weights if they exist\n",
    "if cfg[\"generator_weights\"] is not False:\n",
    "    G.load_weights(cfg[\"generator_weights\"])\n",
    "    print(\"Weights are loaded for G\")\n",
    "print(f\"[Model G] output shape: {sample_G_output.shape}\")\n",
    "\n",
    "G_optimizer = tf.optimizers.Adam(learning_rate=cfg[\"G_learning_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ba4deb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model D] real_fake output shape: (1, 5, 5, 1)\n",
      "[Model D] image output shape(1, 128, 128, 3)\n",
      "[Model D] image part output shape(1, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "# create Discriminator\n",
    "\n",
    "D = sle_gan.Discriminator(cfg[\"resolution\"])\n",
    "sample_D_output = D.initialize(cfg[\"batch_size\"])\n",
    "\n",
    "if cfg[\"generator_weights\"] is not False:\n",
    "    D.load_weights(str(checkpoints_folder / \"D_checkpoint.h5\"))\n",
    "    print(\"Weights are loaded for D\")\n",
    "print(f\"[Model D] real_fake output shape: {sample_D_output[0].shape}\")\n",
    "print(f\"[Model D] image output shape{sample_D_output[1].shape}\")\n",
    "print(f\"[Model D] image part output shape{sample_D_output[2].shape}\")\n",
    "\n",
    "D_optimizer = tf.optimizers.Adam(learning_rate=cfg[\"D_learning_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a13e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg[\"fid\"]:\n",
    "    # Model for the FID calculation\n",
    "    fid_inception_model = sle_gan.InceptionModel(height=cfg[\"resolution\"], width=cfg[\"resolution\"])\n",
    "\n",
    "# what does this do?\n",
    "test_input_size = 25\n",
    "test_input_for_generation = sle_gan.create_input_noise(test_input_size)\n",
    "test_images = sle_gan.get_test_images(test_input_size, cfg[\"data_folder\"], cfg[\"resolution\"])\n",
    "\n",
    "tb_file_writer = tf.summary.create_file_writer(str(logs_folder))\n",
    "tb_file_writer.set_as_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6569c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set loss metrics\n",
    "\n",
    "G_loss_metric = tf.keras.metrics.Mean()\n",
    "D_loss_metric = tf.keras.metrics.Mean()\n",
    "D_real_fake_loss_metric = tf.keras.metrics.Mean()\n",
    "D_I_reconstruction_loss_metric = tf.keras.metrics.Mean()\n",
    "D_I_part_reconstruction_loss_metric = tf.keras.metrics.Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e7c060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set augmentation policies\n",
    "\n",
    "diff_augment_policies = None\n",
    "if cfg[\"diff_augment\"]:\n",
    "    diff_augment_policies = cfg[\"diff_augment_policies\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa1d7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train gan\n",
    "\n",
    "for epoch in range(cfg[\"epochs\"]):\n",
    "    print(\"### Epoch{epoch} ###\".format(epoch))\n",
    "    for step, image_batch in enumerate(dataset):\n",
    "        G_loss, \n",
    "        D_loss, \n",
    "        D_real_fake_loss, \n",
    "        D_I_reconstruction_loss, \n",
    "        D_I_part_reconstruction_loss = sle_gan.train_step(\n",
    "            G=G,\n",
    "            D=D,\n",
    "            G_optimizer=G_optimizer,\n",
    "            D_optimizer=D_optimizer,\n",
    "            images=image_batch,\n",
    "            diff_augmenter_policies=diff_augment_policies)\n",
    "\n",
    "        G_loss_metric(G_loss)\n",
    "        D_loss_metric(D_loss)\n",
    "        D_real_fake_loss_metric(D_real_fake_loss)\n",
    "        D_I_reconstruction_loss_metric(D_I_reconstruction_loss)\n",
    "        D_I_part_reconstruction_loss_metric(D_I_part_reconstruction_loss)\n",
    "\n",
    "        if step % 100 == 0 and step != 0:\n",
    "            print(f\"\\tStep {step} - \"\n",
    "                  f\"G loss {G_loss_metric.result():.4f} | \"\n",
    "                  f\"D loss {D_loss_metric.result():.4f} | \"\n",
    "                  f\"D realfake loss {D_real_fake_loss_metric.result():.4f} | \"\n",
    "                  f\"D I recon loss {D_I_reconstruction_loss_metric.result():.4f} | \"\n",
    "                  f\"D I part recon loss {D_I_part_reconstruction_loss_metric.result():.4f}\")\n",
    "\n",
    "    if cfg[\"fid\"]:\n",
    "        if epoch % cfg[\"fid_frequency\"] == 0:\n",
    "            fid_score = sle_gan.evaluation_step(inception_model=fid_inception_model,\n",
    "                                                dataset=dataset,\n",
    "                                                G=G,\n",
    "                                                batch_size=cfg[\"batch_size\"],\n",
    "                                                image_height=cfg[\"resolution\"],\n",
    "                                                image_width=cfg[\"resolution\"],\n",
    "                                                nb_of_images_to_use=cfg[\"fid_number_of_images\"])\n",
    "            print(f\"[FID] {fid_score:.2f}\")\n",
    "            tf.summary.scalar(\"FID_score\", fid_score, epoch)\n",
    "\n",
    "    tf.summary.scalar(\"G_loss/G_loss\", G_loss_metric.result(), epoch)\n",
    "    tf.summary.scalar(\"D_loss/D_loss\", D_loss_metric.result(), epoch)\n",
    "    tf.summary.scalar(\"D_loss/D_real_fake_loss\", D_real_fake_loss_metric.result(), epoch)\n",
    "    tf.summary.scalar(\"D_loss/D_I_reconstruction_loss\", D_I_reconstruction_loss_metric.result(), epoch)\n",
    "    tf.summary.scalar(\"D_loss/D_I_part_reconstruction_loss\", D_I_part_reconstruction_loss_metric.result(), epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch} - \"\n",
    "          f\"G loss {G_loss_metric.result():.4f} | \"\n",
    "          f\"D loss {D_loss_metric.result():.4f} | \"\n",
    "          f\"D realfake loss {D_real_fake_loss_metric.result():.4f} | \"\n",
    "          f\"D I recon loss {D_I_reconstruction_loss_metric.result():.4f} | \"\n",
    "          f\"D I part recon loss {D_I_part_reconstruction_loss_metric.result():.4f}\")\n",
    "\n",
    "    G_loss_metric.reset_states()\n",
    "    D_loss_metric.reset_states()\n",
    "    D_real_fake_loss_metric.reset_states()\n",
    "    D_I_part_reconstruction_loss_metric.reset_states()\n",
    "    D_I_reconstruction_loss_metric.reset_states()\n",
    "\n",
    "    # TODO: save weights only when the FID score gets better\n",
    "    G.save_weights(str(checkpoints_folder / \"G_checkpoint.h5\"))\n",
    "    D.save_weights(str(checkpoints_folder / \"D_checkpoint.h5\"))\n",
    "\n",
    "    # Generate test images\n",
    "    generated_images = G(test_input_for_generation, training=False)\n",
    "    generated_images = sle_gan.postprocess_images(generated_images, dtype=tf.uint8).numpy()\n",
    "    sle_gan.visualize_images_on_grid_and_save(epoch, generated_images, experiments_folder / \"generated_images\",\n",
    "                                              5, 5)\n",
    "\n",
    "    # Generate reconstructions from Discriminator\n",
    "    _, decoded_images, decoded_part_images = D(test_images, training=False)\n",
    "    decoded_images = sle_gan.postprocess_images(decoded_images, dtype=tf.uint8).numpy()\n",
    "    decoded_part_images = sle_gan.postprocess_images(decoded_part_images, dtype=tf.uint8).numpy()\n",
    "    sle_gan.visualize_images_on_grid_and_save(epoch, decoded_images, experiments_folder / \"reconstructed_whole_images\",\n",
    "                                              5, 5)\n",
    "    sle_gan.visualize_images_on_grid_and_save(epoch, decoded_part_images,\n",
    "                                              experiments_folder / \"reconstructed_part_images\", 5, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
